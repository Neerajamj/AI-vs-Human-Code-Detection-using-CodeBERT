{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13666452,"sourceType":"datasetVersion","datasetId":8689223}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os, warnings\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\nos.environ[\"WANDB_DISABLED\"] = \"true\"\nwarnings.filterwarnings(\"ignore\")\n\nprint(\"Environment ready.\")\n\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-11T05:45:59.376656Z","iopub.execute_input":"2025-11-11T05:45:59.377391Z","iopub.status.idle":"2025-11-11T05:45:59.381775Z","shell.execute_reply.started":"2025-11-11T05:45:59.377365Z","shell.execute_reply":"2025-11-11T05:45:59.380991Z"}},"outputs":[{"name":"stdout","text":"Environment ready.\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"import os\nprint(os.listdir(\"/kaggle/input/binary-code-detection-a\"))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-11T05:45:59.448519Z","iopub.execute_input":"2025-11-11T05:45:59.448719Z","iopub.status.idle":"2025-11-11T05:45:59.467461Z","shell.execute_reply.started":"2025-11-11T05:45:59.448704Z","shell.execute_reply":"2025-11-11T05:45:59.466759Z"}},"outputs":[{"name":"stdout","text":"['label_to_id (1).json', 'id_to_label (1).json', 'task_a_trial (1).parquet']\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"import pandas as pd\n\nPATH = \"/kaggle/input/binary-code-detection-a/task_a_trial (1).parquet\"\ndf = pd.read_parquet(PATH)\n\nprint(df.columns)\nprint(len(df))\nprint(df.head(2))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-11T05:45:59.468728Z","iopub.execute_input":"2025-11-11T05:45:59.468992Z","iopub.status.idle":"2025-11-11T05:45:59.516436Z","shell.execute_reply.started":"2025-11-11T05:45:59.468972Z","shell.execute_reply":"2025-11-11T05:45:59.515685Z"}},"outputs":[{"name":"stdout","text":"Index(['code', 'generator', 'label', 'language'], dtype='object')\n10000\n                                                      code  \\\n991293   #include <iostream>\\n#include <string>\\n#inclu...   \n1044782  #include <bits/stdc++.h>\\n\\n\\n\\n// #include <e...   \n\n                                  generator  label language  \n991293   microsoft/Phi-3-medium-4k-instruct      0      C++  \n1044782                               Human      1      C++  \n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"import os, warnings, torch\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\nos.environ[\"WANDB_DISABLED\"] = \"true\"   # avoid wandb login\nwarnings.filterwarnings(\"ignore\")\n\nprint(\"PyTorch:\", torch.__version__, \"| CUDA available:\", torch.cuda.is_available())\nif torch.cuda.is_available():\n    print(\"Device name:\", torch.cuda.get_device_name(0))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-11T05:45:59.517179Z","iopub.execute_input":"2025-11-11T05:45:59.517528Z","iopub.status.idle":"2025-11-11T05:45:59.523639Z","shell.execute_reply.started":"2025-11-11T05:45:59.517500Z","shell.execute_reply":"2025-11-11T05:45:59.522888Z"}},"outputs":[{"name":"stdout","text":"PyTorch: 2.6.0+cu124 | CUDA available: True\nDevice name: Tesla T4\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"import os, re, unicodedata\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score, classification_report, confusion_matrix\n\nimport torch\nfrom torch.utils.data import Dataset\n\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForSequenceClassification,\n    TrainingArguments,\n    Trainer,\n)\nimport transformers\nprint(\"Transformers:\", transformers.__version__)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-11T05:45:59.524395Z","iopub.execute_input":"2025-11-11T05:45:59.524633Z","iopub.status.idle":"2025-11-11T05:45:59.550519Z","shell.execute_reply.started":"2025-11-11T05:45:59.524610Z","shell.execute_reply":"2025-11-11T05:45:59.549758Z"}},"outputs":[{"name":"stdout","text":"Transformers: 4.53.3\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"DATA_DIR = Path(\"/kaggle/input/binary-code-detection-a\")\nPARQUET = DATA_DIR / \"task_a_trial (1).parquet\"  # <- your exact file\n\ndf = pd.read_parquet(PARQUET)\nprint(\"Columns:\", list(df.columns), \"Shape:\", df.shape)\ndisplay(df.head(2))\n\n# Ensure required columns exist\nassert \"code\" in df.columns, \"Expected a 'code' column.\"\nassert \"label\" in df.columns, \"Expected a 'label' (0/1 or 'human'/'machine') column.\"\n\n# Make numeric labels: 0=human, 1=AI\nif df[\"label\"].dtype == object:\n    mapping = {\"human\":0, \"machine\":1, \"ai\":1, \"Human\":0, \"Machine\":1, \"AI\":1}\n    df[\"y\"] = df[\"label\"].map(mapping).astype(int)\nelse:\n    df[\"y\"] = df[\"label\"].astype(int)\n\n# Drop NAs just in case\ndf = df.dropna(subset=[\"code\",\"y\"]).reset_index(drop=True)\nprint(\"After cleaning:\", df.shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-11T05:45:59.552009Z","iopub.execute_input":"2025-11-11T05:45:59.552460Z","iopub.status.idle":"2025-11-11T05:45:59.636977Z","shell.execute_reply.started":"2025-11-11T05:45:59.552444Z","shell.execute_reply":"2025-11-11T05:45:59.636389Z"}},"outputs":[{"name":"stdout","text":"Columns: ['code', 'generator', 'label', 'language'] Shape: (10000, 4)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"                                                      code  \\\n991293   #include <iostream>\\n#include <string>\\n#inclu...   \n1044782  #include <bits/stdc++.h>\\n\\n\\n\\n// #include <e...   \n\n                                  generator  label language  \n991293   microsoft/Phi-3-medium-4k-instruct      0      C++  \n1044782                               Human      1      C++  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>code</th>\n      <th>generator</th>\n      <th>label</th>\n      <th>language</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>991293</th>\n      <td>#include &lt;iostream&gt;\\n#include &lt;string&gt;\\n#inclu...</td>\n      <td>microsoft/Phi-3-medium-4k-instruct</td>\n      <td>0</td>\n      <td>C++</td>\n    </tr>\n    <tr>\n      <th>1044782</th>\n      <td>#include &lt;bits/stdc++.h&gt;\\n\\n\\n\\n// #include &lt;e...</td>\n      <td>Human</td>\n      <td>1</td>\n      <td>C++</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"name":"stdout","text":"After cleaning: (10000, 5)\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"def normalize_code(s: str) -> str:\n    s = unicodedata.normalize(\"NFKC\", str(s))\n    s = s.replace(\"\\r\\n\",\"\\n\").replace(\"\\r\",\"\\n\")\n    s = re.sub(r\"[ \\t]+\", \" \", s)\n    s = re.sub(r\"\\n{3,}\", \"\\n\\n\", s).strip()\n    return s\n\ndf[\"code_norm\"] = df[\"code\"].apply(normalize_code)\n\ntrain_df, valid_df = train_test_split(\n    df, test_size=0.2, random_state=42, stratify=df[\"y\"]\n)\nprint(\"Train:\", train_df.shape, \"Valid:\", valid_df.shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-11T05:45:59.637685Z","iopub.execute_input":"2025-11-11T05:45:59.637947Z","iopub.status.idle":"2025-11-11T05:46:00.097897Z","shell.execute_reply.started":"2025-11-11T05:45:59.637919Z","shell.execute_reply":"2025-11-11T05:46:00.097004Z"}},"outputs":[{"name":"stdout","text":"Train: (8000, 6) Valid: (2000, 6)\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"MODEL_NAME = \"microsoft/codebert-base\"\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n\nclass CodeClsDataset(Dataset):\n    def __init__(self, texts, labels, tokenizer, max_len=512):\n        self.texts = list(texts)\n        self.labels = list(labels)\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n    def __len__(self): return len(self.texts)\n    def __getitem__(self, idx):\n        enc = self.tokenizer(\n            self.texts[idx],\n            padding=\"max_length\",\n            truncation=True,\n            max_length=self.max_len,\n            return_tensors=\"pt\",\n        )\n        item = {k: v.squeeze(0) for k, v in enc.items()}\n        item[\"labels\"] = torch.tensor(int(self.labels[idx]), dtype=torch.long)\n        return item\n\ndtrain = CodeClsDataset(train_df[\"code_norm\"], train_df[\"y\"], tokenizer)\ndvalid = CodeClsDataset(valid_df[\"code_norm\"], valid_df[\"y\"], tokenizer)\nlen(dtrain), len(dvalid)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-11T05:46:00.098829Z","iopub.execute_input":"2025-11-11T05:46:00.099173Z","iopub.status.idle":"2025-11-11T05:46:00.440811Z","shell.execute_reply.started":"2025-11-11T05:46:00.099143Z","shell.execute_reply":"2025-11-11T05:46:00.440177Z"}},"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"(8000, 2000)"},"metadata":{}}],"execution_count":21},{"cell_type":"code","source":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2).to(device)\nmodel.config.problem_type = \"single_label_classification\"\n\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    preds = np.argmax(logits, axis=-1)\n    return {\"macro_f1\": f1_score(labels, preds, average=\"macro\")}\n\nargs_kw = dict(\n    output_dir=\"codebert_out\",\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=16,\n    learning_rate=2e-5,\n    num_train_epochs=3,\n    save_strategy=\"epoch\",\n    load_best_model_at_end=True,\n    metric_for_best_model=\"macro_f1\",\n    greater_is_better=True,\n    fp16=True,\n    report_to=\"none\",  # no wandb\n)\ntry:\n    training_args = TrainingArguments(evaluation_strategy=\"epoch\", **args_kw)\nexcept TypeError:\n    training_args = TrainingArguments(eval_strategy=\"epoch\", **args_kw)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=dtrain,\n    eval_dataset=dvalid,\n    compute_metrics=compute_metrics,\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-11T05:46:00.441500Z","iopub.execute_input":"2025-11-11T05:46:00.441752Z","iopub.status.idle":"2025-11-11T05:46:01.682045Z","shell.execute_reply.started":"2025-11-11T05:46:00.441733Z","shell.execute_reply":"2025-11-11T05:46:01.681443Z"}},"outputs":[{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/codebert-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"trainer.train()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-11T05:46:01.683219Z","iopub.execute_input":"2025-11-11T05:46:01.683633Z","iopub.status.idle":"2025-11-11T06:11:43.362770Z","shell.execute_reply.started":"2025-11-11T05:46:01.683614Z","shell.execute_reply":"2025-11-11T06:11:43.362123Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1500' max='1500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1500/1500 25:40, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Macro F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.378800</td>\n      <td>0.282935</td>\n      <td>0.893483</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.237800</td>\n      <td>0.266978</td>\n      <td>0.902913</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.172300</td>\n      <td>0.275037</td>\n      <td>0.916496</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=1500, training_loss=0.2629658660888672, metrics={'train_runtime': 1541.0793, 'train_samples_per_second': 15.574, 'train_steps_per_second': 0.973, 'total_flos': 6314665328640000.0, 'train_loss': 0.2629658660888672, 'epoch': 3.0})"},"metadata":{}}],"execution_count":23},{"cell_type":"code","source":"from sklearn.metrics import f1_score, classification_report, confusion_matrix\n\neval_res = trainer.evaluate()\nprint(\"FINAL VALID MACRO F1:\", eval_res.get(\"eval_macro_f1\"))\n\ndef predict_batched(ds, batch_size=64):\n    preds = []\n    for i in range(0, len(ds), batch_size):\n        batch = [ds[j] for j in range(i, min(i+batch_size, len(ds)))]\n        input_ids = torch.stack([b[\"input_ids\"] for b in batch]).to(device)\n        attn = torch.stack([b[\"attention_mask\"] for b in batch]).to(device)\n        with torch.no_grad():\n            logits = model(input_ids=input_ids, attention_mask=attn).logits\n        preds.extend(torch.argmax(logits, dim=-1).cpu().tolist())\n    return np.array(preds)\n\ny_true = valid_df[\"y\"].to_numpy()\ny_pred = predict_batched(dvalid)\n\nprint(\"\\nConfusion matrix:\\n\", confusion_matrix(y_true, y_pred))\nprint(\"\\nReport:\\n\", classification_report(y_true, y_pred, target_names=[\"human(0)\",\"AI(1)\"]))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-11T06:11:43.363500Z","iopub.execute_input":"2025-11-11T06:11:43.363717Z","iopub.status.idle":"2025-11-11T06:13:17.846228Z","shell.execute_reply.started":"2025-11-11T06:11:43.363700Z","shell.execute_reply":"2025-11-11T06:13:17.845402Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [63/63 00:34]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"FINAL VALID MACRO F1: 0.916496471975941\n\nConfusion matrix:\n [[910  86]\n [ 81 923]]\n\nReport:\n               precision    recall  f1-score   support\n\n    human(0)       0.92      0.91      0.92       996\n       AI(1)       0.91      0.92      0.92      1004\n\n    accuracy                           0.92      2000\n   macro avg       0.92      0.92      0.92      2000\nweighted avg       0.92      0.92      0.92      2000\n\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"def detect_language(snippet: str) -> str:\n    s = snippet.lower()\n\n    # C++\n    if \"#include\" in s or \"std::\" in s or \"cout<<\" in s or \"cout <<\" in s or \"cin>>\" in s or \"cin >>\" in s:\n        return \"C++\"\n    # C (printf/scanf and NO std::)\n    if \"#include\" in s and (\"printf(\" in s or \"scanf(\" in s) and \"std::\" not in s:\n        return \"C\"\n    # C#\n    if \"using system;\" in s or \"console.writeline\" in s:\n        return \"C#\"\n    # Python\n    if \"def \" in s or \"print(\" in s or \"import \" in s or \"except:\" in s or \"try:\" in s:\n        return \"Python\"\n    # Java\n    if \"public static void main\" in s or \"system.out.println\" in s:\n        return \"Java\"\n    # JavaScript\n    if \"console.log(\" in s or \"function(\" in s or \"=>\" in s or \"var \" in s or \"let \" in s or \"const \" in s:\n        return \"JavaScript\"\n    # PHP\n    if \"<?php\" in s or (\"echo \" in s and \"$\" in s):\n        return \"PHP\"\n    # Go\n    if \"package main\" in s or \"fmt.\" in s or \"func main()\" in s:\n        return \"Go\"\n    # Ruby\n    if re.search(r\"\\bdef\\b.*\\bend\\b\", s) or s.strip().startswith(\"puts \"):\n        return \"Ruby\"\n\n    return \"Unknown\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-11T06:13:17.848605Z","iopub.execute_input":"2025-11-11T06:13:17.848848Z","iopub.status.idle":"2025-11-11T06:13:17.855377Z","shell.execute_reply.started":"2025-11-11T06:13:17.848829Z","shell.execute_reply":"2025-11-11T06:13:17.854630Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"def predict_texts(texts, batch_size=128):\n    preds = []\n    for i in range(0, len(texts), batch_size):\n        batch = list(texts[i:i+batch_size])\n        enc = tokenizer(batch, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n        enc = {k: v.to(device) for k, v in enc.items()}\n        with torch.no_grad():\n            logits = model(**enc).logits\n        preds.extend(torch.argmax(logits, dim=-1).cpu().tolist())\n    return np.array(preds, dtype=int)\n\n# 1) predictions for ALL rows\npred_all = predict_texts(df[\"code_norm\"])\n\n# 2) generator: keep existing if present; else derive from prediction\nif \"generator\" in df.columns:\n    gen_col = df[\"generator\"].astype(str)\nelse:\n    gen_col = pd.Series(np.where(pred_all==0, \"human\", \"AI\"), index=df.index)\n\n# 3) language: use existing if present; else detect\nif \"language\" in df.columns:\n    lang_col = df[\"language\"].astype(str)\nelse:\n    lang_col = df[\"code\"].apply(detect_language)\n\n# a) EXACT 4 columns you requested (label = prediction)\nfinal_predictions = pd.DataFrame({\n    \"code\": df[\"code\"],\n    \"label\": pred_all,             # predicted label (0=human,1=AI)\n    \"generator\": gen_col,\n    \"language\": lang_col\n})\nfinal_predictions.to_csv(\"/kaggle/working/final_predictions.csv\", index=False)\nprint(\"Saved /kaggle/working/final_predictions.csv\")\n\n# b) ALSO a file with true label + prediction (for analysis)\nwith_true_and_pred = pd.DataFrame({\n    \"code\": df[\"code\"],\n    \"label_true\": df[\"y\"].astype(int),\n    \"label_pred\": pred_all,\n    \"generator\": gen_col,\n    \"language\": lang_col\n})\nwith_true_and_pred.to_csv(\"/kaggle/working/with_true_and_pred.csv\", index=False)\nprint(\"Saved /kaggle/working/with_true_and_pred.csv\")\n\ndisplay(final_predictions.head(3))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-11T06:13:17.856251Z","iopub.execute_input":"2025-11-11T06:13:17.856560Z","iopub.status.idle":"2025-11-11T06:18:08.872564Z","shell.execute_reply.started":"2025-11-11T06:13:17.856539Z","shell.execute_reply":"2025-11-11T06:18:08.871753Z"}},"outputs":[{"name":"stdout","text":"Saved /kaggle/working/final_predictions.csv\nSaved /kaggle/working/with_true_and_pred.csv\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"                                                code  label  \\\n0  #include <iostream>\\n#include <string>\\n#inclu...      0   \n1  #include <bits/stdc++.h>\\n\\n\\n\\n// #include <e...      1   \n2  class node:\\n    value = 0\\n    index = -1\\n  ...      1   \n\n                             generator language  \n0   microsoft/Phi-3-medium-4k-instruct      C++  \n1                                Human      C++  \n2  ibm-granite/granite-8b-code-base-4k   Python  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>code</th>\n      <th>label</th>\n      <th>generator</th>\n      <th>language</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>#include &lt;iostream&gt;\\n#include &lt;string&gt;\\n#inclu...</td>\n      <td>0</td>\n      <td>microsoft/Phi-3-medium-4k-instruct</td>\n      <td>C++</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>#include &lt;bits/stdc++.h&gt;\\n\\n\\n\\n// #include &lt;e...</td>\n      <td>1</td>\n      <td>Human</td>\n      <td>C++</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>class node:\\n    value = 0\\n    index = -1\\n  ...</td>\n      <td>1</td>\n      <td>ibm-granite/granite-8b-code-base-4k</td>\n      <td>Python</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":26},{"cell_type":"code","source":"def predict_single(code: str):\n    enc = tokenizer(code, padding=\"max_length\", truncation=True, max_length=512, return_tensors=\"pt\")\n    enc = {k: v.to(device) for k, v in enc.items()}\n    with torch.no_grad():\n        pred = int(model(**enc).logits.argmax(-1).item())\n    gen = \"human\" if pred==0 else \"AI\"\n    lang = code if isinstance(code, str) else \"\"\n    lang = detect_language(code) if \"language\" not in df.columns else None  # if your df has language, you may ignore this\n    return {\"code\": code, \"label\": pred, \"generator\": gen, \"language\": detect_language(code)}\n\n# example\nex = \"\"\"\nfor(int i=0;i<n;i++){\n    cout << a[i] << endl;\n}\n\n\"\"\"\npredict_single(ex)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-11T06:18:08.873773Z","iopub.execute_input":"2025-11-11T06:18:08.874065Z","iopub.status.idle":"2025-11-11T06:18:08.903543Z","shell.execute_reply.started":"2025-11-11T06:18:08.874039Z","shell.execute_reply":"2025-11-11T06:18:08.902674Z"}},"outputs":[{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"{'code': '\\nfor(int i=0;i<n;i++){\\n    cout << a[i] << endl;\\n}\\n\\n',\n 'label': 0,\n 'generator': 'human',\n 'language': 'C++'}"},"metadata":{}}],"execution_count":27},{"cell_type":"code","source":"ex1 = \"\"\"\n\na = 10\nb = 20\n\nsum = a + b\n\nprint(\"Sum =\", sum)\n\n\n\"\"\"\npredict_single(ex1)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-11T06:18:09.007750Z","iopub.execute_input":"2025-11-11T06:18:09.008022Z","iopub.status.idle":"2025-11-11T06:18:09.041221Z","shell.execute_reply.started":"2025-11-11T06:18:09.008002Z","shell.execute_reply":"2025-11-11T06:18:09.040677Z"}},"outputs":[{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"{'code': '\\n# simple python code example\\n\\na = 10\\nb = 20\\n\\nsum = a + b\\n\\nprint(\"Sum =\", sum)\\n\\n\\n',\n 'label': 0,\n 'generator': 'human',\n 'language': 'Python'}"},"metadata":{}}],"execution_count":31}]}